import requests
import os
import urllib3
import threading
import time
from pyquery import PyQuery as pq
from urllib.parse import urljoin
from requests.exceptions import ReadTimeout, ConnectionError

class Malware:
    def __init__(self):
        # 请求头
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.87 Safari/537.36',
        }
        # url列表，用于去重
        self.url_list = []
        # 文件存放路径
        self.path = os.getcwd() + os.sep + 'files' + os.sep
        # 忽略SSL警告
        urllib3.disable_warnings()

    def parse_index(self, year):   #解析年份索引页

        print(f'正在获取【{year}】年的数据...')
        try:
            url = f'https://www.malware-traffic-analysis.net/{year}/index.html'
            response = requests.get(url, headers=self.headers, timeout=15)
            if response.status_code == 200:
                doc = pq(response.text)
                # 获取每一篇文章链接
                links = doc('.content > ul > li > a:nth-child(1)').items()
                for link in links:
                    yield urljoin(url, link.attr('href'))
        except (ReadTimeout, ConnectionError) as e:
            print('连接异常，重试', e.args)
            self.parse_index(year)

    def parse_detail(self, url):   #解析详情页

        print(f'正在请求链接：{url}')
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            if response.status_code == 200:
                doc = pq(response.text)
                # 获取每一个链接的url
                links = doc('.menu_link').items()
                for link in links:
                    # 获取下载压缩包链接
                    if 'pcap' in link.text():
                        href = urljoin(url, link.attr('href'))
                        # 去重
                        if href not in self.url_list:
                            self.url_list.append(href)
                            yield href
        except (ReadTimeout, ConnectionError) as e:
            print('连接异常，重试', e.args)
            self.parse_detail(url)

    def download_file(self, url):   #下载文件

        filename = url.split('/')[-1]
        file_path = self.path + filename

        try:
            # 已下载的文件不重复下载
            if not os.path.exists(file_path):
                print(f'正在下载文件：{filename}')
                response = requests.get(url, headers=self.headers, timeout=20)
                if response.status_code == 200:
                    with open(file_path, 'wb') as f:
                        f.write(response.content)
                        print('下载完成')
            else:
                print(f'文件：{filename}已存在')
        except (ReadTimeout, ConnectionError) as e:
            print('连接异常，重试', e.args)
            self.download_file(url)

    def run(self):
        # 若存放文件的文件夹不存在，则创建文件夹
        if not os.path.exists(self.path):
            os.mkdir(self.path)

        #year = 2020
        for year in range(2013, 2021):
            for url in self.parse_index(year):
                threads = []
                for href in self.parse_detail(url):
                    thread = threading.Thread(target=self.download_file, args=(href,))
                    threads.append(thread)
                    thread.start()
                for t in threads:
                    t.join()


if __name__ == '__main__':
    malware = Malware()
    malware.run()

